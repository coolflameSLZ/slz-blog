---
title: 注册中心的设计与选型
toc: true
categories:
  - 后端
tags:
  - 注册中心
  - 设计
hide: true
date: 2021-07-31 23:47:39
---

这是摘要
<!-- more -->

------



# 注册中心的设计与选型



## 没有注册中心的解决方案

- 通过全局配置文件，来规定服务ip的调用关系。<br>迁移、扩容的时候非常痛苦。



## 注册中心，需要实现的点



### 注册中心的主要功能

#### 获取服务信息

- 路由信息，服务注册节点的IP，端口。
- 服务元数据信息，序列化协议，负载均衡规则，节点权重等



#### 服务发现：

- 启动时拉：消费方启动后，先从注册中心获取提供方的节点列表

- 通知回调：提供方的节点变更时，主动调用消费方，让消费方重新拉取节点数据

- 轮询拉取：回调不一定总是成功，所以需要兜底策略：轮询，分钟级别。

  

#### 主动通知调用方，服务节点发生变更

#### 机器迁移

#### 权重

#### 灰度流量



#### 健康检查

- 一般情况下的服务失效原因

  1. 部署重启，正常下线，可以主动通知注册中心
  2. 服务异常终止，比如机器掉电，无法主动通知下线事件
  3. 服务假死（注册中心线程可用，但工作线程不可用，表现起来就是依然可用向注册中心发心跳，但业务接口已经卡死不可用。）

  

- 注册中心监控不健康节点的方案

  - 主动下线，能解决1 
  - 心跳上报，能解决 1，2
  - 注册中心，主动探活，探测工作线程是否可用。能解决 1，2，3
  - consumer 通过正常调用，也可以感知到对方服务是否假死。<br>也通过consumer上报异常，解决  1，2，3

  

## 注册中心的实现方案

### 业务模型

- 消费者视角，主要关心的是服务的提供者的ip地址。
- 提供者的角度，主要关系的是，本服务被哪些消费者注册消费了，方便提供者节点上下线的时候，主动通知消费者

```json
{
    "providerList": [
        {
            "serverName": "orderSvr",
            "consumerList": "userSvr,addrSvr,productSvr"
        },
        {
            "serverName": "addrSvr",
            "consumerList": "userSvr,addrSvr,productSvr"
        }
    ],
    "consumerList": [
        {
            "serverName": "userSvr",
            "providerList": "orderSvr,addrSvr,productSvr"
        },
        {
            "serverName": "orderSvr",
            "providerList": "userSvr,addrSvr,productSvr"

        }
    ],
    "ipList": [
        {
            "serverName": "orderSvr",
            "ipList": "xxxx,xxxx,xxxx",
            "port": "8081"
        },
        {
            "serverName": "userSvr",
            "ipList": "xxxx,xxxx,xxxx",
            "port": "8082"
        }
    ]
}
```



### 超时处理

- 遍历扫描，以前的微服务注册中心，使用该方法即可，因为节点不多。

- 动态分组算法，节点数量较多时使用，一般应用在 IM 中 长连接 keepalive 超时主动清理的扫描机制， 节点以10万为单位。遍历肯定不行

  <img src="https://cdn.jsdelivr.net/gh/coolflameSLZ/img/img20210801012443.png" alt="image-20210801012442966" style="zoom:33%;" />



1. 该数据模型，如果n秒超时，有n个bucket。比如1分钟超时，就有60个bucket。

2. 这样每一个连接的超时时间，就能确定的放到某一个bucket里面，比如现在是53秒，有一个新的连接进来，那么新的连接就要放到。

3. 有一个游标一直轮询bucket，每一秒往后移动一格，当53秒的时候，移动到52号格子里，然后清理52号格子。

   

### 变更通知

#### gossip协议

- 六度分离理论，周期性散播下线通知。
- 随机选择n个节点，传递新数据，传x轮。每个节点获取新数据后，仍然选n个节点，传新数据，传x轮。
- 避免不了重复传递，但无所谓。
- 经过x/n轮后，所有节点已经更新数据。





## 开源注册中心选型



### CAP

本质上，注册中心是一个高可用，高数据一致性的分布式存储，里面存了各个节点的数据。

那么分布式cun存储，就要搬出CAP理论了：

- （partition toleran）数据可靠性，数据是冗余存储的，不会因为单节点故障导致数据整体丢失。
- （consistency）数据一致性，个节点间的数据，最终会保证统一。
- （availability）服务整体可用性，服务不间断对外提供服务。



### AP or CP

- AP，牺牲一致性，保证不间断对外提供服务。<br>代表组件：**eureka，nacos**
- CP，优先保证数据一致性，当发现数据不一致的时候，重新选举，然后同步数据；<br>在这期间，停止对外服务 <br>代表组件：**zookeeper**

- 总结：

  如果是**注册中心**：一定选取AP模式，对于注册中心这个产品，节点状态数据变动不频繁。<br>注册中心不能因为自身的任何原因，让程序变得不可用，这是注册中心设计应该遵循的铁律<br>注册中心，如果节点变动超过阈值，甚至要主动报警；并防止节点下线（保护模式）。

  如果是**分布式协调器**，分布式锁。则需要使用CP模式。



### 注册中心的综合对比

| 特征               | zookeeper | etcd     | consul   | eureka   | nacos                  |
| ------------------ | --------- | :------- | -------- | -------- | ---------------------- |
| 服务健康检查       | 长连接    | 心跳     | 心跳     | 心跳     | 支持                   |
| 多数据中心         | \--       | \--      | 支持     | \--      | 支持                   |
| kv存储服务         | 支持      | 支持     | 支持     | \--      | 支持                   |
| 一致性             | Zab       | raft     | raft     | 弱一致性 | Distro                 |
| CAP定理            | CP        | CP       | CP       | AP       | AP                     |
| 服务端<br>主动探活 | 私有协议  | 私有协议 | 支持     | 长轮询   | 长轮询<br>v2使用长连接 |
| 客户端访问         | SDK       | http     | http&dns | http     | http                   |
| 社区支持           | 积极      | 积极     | 积极     | 不积极   | 积极                   |









## Nacos注册中心深入分析

### 健康检查

- 临时节点：心跳注册机制，ttl超时下线。
- 持久化节点，比如数据库，使用服务端探活机制，标记不可用。如果不可用超过阈值，则报警。
- 心跳阈值：5秒，上报；15秒，标记不健康；30秒，剔除节点

### 数据模型

- 数据存储，IP地址，端口，健康检查，ttl，权重。

- 数据隔离，多重命名空间，User，NameSpace，Group

  User：多租户

  NameSpace：业务线隔离

  Group：A，B集群，平滑上线使用。

### 数据一致性保障

- Raft CP一致性	【一般适不使用该模式】
- Distro AP一致性 【无主模式】

<img src="https://cdn.jsdelivr.net/gh/coolflameSLZ/img/img20210802234926.png" alt="image-20210802234926080" style="zoom: 33%;" />





节点上线逻辑：【】

节点下线逻辑：【】



## Zookeeper实现深入剖析



### 选主逻辑

#### zk节点的角色

- leader，响应写入请求，发起提案，超过半数follower同意写入，则写入成功。
- follower，响应查询，将写入请求转发给leader，参与选举 和 投票 以及写入操作。
- observer，响应查询，将写入请求转发给leader，不参与投票，只负责接收写入操作。
- leader，代表公司ceo，只输出提案，并最终确认提案；<br>follower，代表管理层，转发提案并参与投票，最终执行提案；<br>observer，代表一线员工，转发提案，不参与投票，执行提案。

#### 选主规则

- 获得法定数量的票数，即follower 数量过半。

- 判断依据

  - Epoch：leader的任期
  - ZXID：zookeeper的事务ID，越大表示事务越新
  - SID：集群的每个节点唯一编号
  - 比较策略：连续排序，根据 任期、ZXID、SID 三个值，做比较。大的胜利。

- 选主逻辑

  - 节点进入 looking 状态

  - 广播发起投票，选出 最大 的节点，发起二次投票

  - 超过半数成为主。

    

### 数据一致性保障

- zab协议， todo。



### 数据模型

树状结构存储数据，分为永久节点和临时节点

- DataNode ，Zookeeper中存储的最小单元，是持久化数据节点描述的最小单位

```java
DataNode parent;  		//父节点的引用 
byte data[]; 					//该节点存储的数据
Long acl;							//acl控制权限
StatPersisted stat; 	//持久化节点状态
Set<String> children; //子节点列表
```

- DataTree，以树形结构存储了zookeeper中所有的数据信息

```java
ConcurrentHashMap<String, DataNode> nodes;	//当前树存的节点
WatchManager dataWatches;										//数据变更通知
WatchManager childWatches;									//节点变更通知
String rootZookeeper;												//根节点
Map<Long, HashSet<String>> ephemerals				//临时节点信息
```

- ZKDataBase，负责管理Zookeeper的数据、会话信息和事务日志

```java
DataTree dataTree;																			//这zk的这棵树
ConcurrentHashMap<Long, Integer> sessionsWithTimeouts; 	//客户端会话连接管理 FileTxnSnapLog snapLog; 																//事务日志
```

